{"cells":[{"cell_type":"markdown","source":["#### Prerequisites\n","\n","- Basic familiarity with [Numpy](https://numpy.org/doc/stable/user/quickstart.html)\n","- Basic familiarity with [Pyplot](https://matplotlib.org/stable/tutorials/introductory/pyplot.html)"],"metadata":{"id":"Vr8eifSe1FQi"}},{"cell_type":"markdown","source":["## Outline\n","\n","- [Section 0](#section-0): NumPy Tips and Code Clarity\n","- [Section 1](#section-1): Intro to Linear Regression\n","- [Section 2](#section-2): Least Squared Loss and Maximum Likelihood\n","- [Section 3](#section-3): Ridge Regression\n","- [Section 4](#section-4): LASSO Regression\n","\n"],"metadata":{"id":"Ml6P8gUcTqwl"}},{"cell_type":"markdown","source":["<a name=\"section-0\"></a>\n","\n","# Section 0: NumPy Tips and Code Clarity\n","\n","\n","There are multiple ways in NumPy to do each of the following basic operations:\n","\n","- Matrix-matrix and matrix-vector product.\n","- Matrix-matrix and vector-vector element-wise product.\n","- vector-vector inner and outer products.\n","\n","Avoid using general functions such as `np.dot` that handles most of these operations depending on the shapes of the input parameters.\n","\n","\n","Note: to check a function documentation, you can do that inside a Nootbook cell using `?<function_name>`.\n"],"metadata":{"id":"FaDZSaJJIw4N"}},{"cell_type":"code","source":["import numpy as np\n","?np.dot"],"metadata":{"id":"LQsn3a3bImnV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# shape: (2, 2)\n","A = np.array([[1, 2], [3, 4]])\n","\n","# shape: (2, 2)\n","B = np.array([[5, 6], [7, 8]])\n","\n","# shape: (2, )\n","v1 =  np.array([5, 6])\n","\n","# shape: (3, )\n","v2 = np.array([4, 5, 6])\n","\n","# shape: (2, )\n","v3 = np.arange(8, 11)\n","\n"],"metadata":{"id":"ImU9lXymIm-N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result_matrix_matrix = np.dot(A, B)\n","# result_matrix_matrix = A @ B # Better\n","\n","\n","result_matrix_matrix"],"metadata":{"id":"jPJvKRjmPDYR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","result_matrix_vector = np.dot(A, v1)\n","# result_matrix_vector = A @ v1 # Better\n","result_matrix_vector"],"metadata":{"id":"4h13M_1MN2_a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result_inner_product = np.dot(v2, v3)\n","# result_inner_product = np.inner(v2, v3)\n","result_inner_product"],"metadata":{"id":"6jTafZCkN3Lx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**There are several issues in using `np.dot` in the previous cells**. It can be very confusing to read a code with `np.dot` as you are trying to understand what operation is actually intended. The reader is required first to read the documentation of `np.dot` and then probe the shape of the input parameters to interpret the expression.\n","\n","\n","To avoid confusion, consider the following practice:\n","\n","- Use the explicit `@` operator for matrix-matrix and matrix-vector products.\n","- Use the explicit `*` operator for element-wise products or [broadcasted products](https://numpy.org/doc/stable/user/basics.broadcasting.html).\n","- Use the explicit `np.inner` function for inner product between 1-D vectors, same for `np.outer` function."],"metadata":{"id":"rvEL7m-sMdbd"}},{"cell_type":"markdown","source":["When you intend to work with an object as a 1-D vector, make sure you don't have excessive dimensions with size 1 of your array."],"metadata":{"id":"WtFPrcKXSjR2"}},{"cell_type":"code","source":["# shape: (1, 2)\n","v4 = np.array([[5, 6]])\n","\n","# shape: (2, 1)\n","v5 = np.array([[5], [6]])\n","\n","# This works due to broadcasting rules.\n","v4 * v5"],"metadata":{"id":"7SgGI74oTIZs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["f'v4.shape: {v4.shape}, v5.shape: {v5.shape},  v4.ndim: {v4.ndim},  v5.ndim: {v5.ndim}'"],"metadata":{"id":"PL74DMtdT0Fr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To fix this and deal with them as vectors:"],"metadata":{"id":"gBsFJJ5HTef5"}},{"cell_type":"code","source":["v4 = v4.squeeze() # Now shape: (2, )\n","v5 = v5.squeeze() # Now shape: (2, )\n","\n","f'v4.shape: {v4.shape}, v5.shape: {v5.shape},  v4.ndim: {v4.ndim},  v5.ndim: {v5.ndim}'\n"],"metadata":{"id":"0QeY9-QgTbdM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["v4 * v5"],"metadata":{"id":"G-1rcHHfULV8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["General practice to improve readability and avoid unexpected behaviour:\n","\n","- When possible use explicit expressions instead of general functions like `np.dot`.\n","- Ensure objects representing 1-D vectors have `ndim==1`. If you are writing a function that deals with vectors and parameters use `assert` statements to make sure the input parameters match what you expect.\n","- Never use the deprectated `np.matrix` class but always `np.array`."],"metadata":{"id":"lB1Mr5nfUSJL"}},{"cell_type":"markdown","metadata":{"id":"VzLYzhtDt9Gl"},"source":["# Linear Regression\n","\n","\n","<a name=\"section-1\"></a>\n","\n","##  Section 1: Intro to Linear regression\n","\n","*Partly adapted from [Deisenroth, Faisal, Ong (2020)](https://mml-book.github.io/).*\n","\n","The purpose of this notebook is to implement linear regression and explore some of its properties, relying mostly on the Python packages numpy and matplotlib.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bgdhMY0gu00z"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Initial global configuration for matplotlib - this is useful to have high quality plots, avoiding to set these parameters for each plot\n","SMALL_SIZE = 12\n","MEDIUM_SIZE = 16\n","BIGGER_SIZE = 20\n","\n","plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n","plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n","plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n","plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n","plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n","plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n","plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"]},{"cell_type":"markdown","metadata":{"id":"wrdWPxiMrRvu"},"source":["We first consider a linear regression problem of the form\n","$$\n","y = \\boldsymbol x^T\\boldsymbol\\beta + \\epsilon\\,,\\quad \\epsilon \\sim \\mathcal N(0, \\sigma^2)\n","$$\n","where $\\boldsymbol x\\in\\mathbb{R}^{p}$ are inputs (i.e., a datapoint with $p$ features) and $y\\in\\mathbb{R}$ are noisy outcomes. The parameter vector $\\boldsymbol\\beta\\in\\mathbb{R}^{p}$ parametrizes the function.\n","\n","We assume we have a training set $(\\boldsymbol x^{(i)}, y^{(i)})$, $i=1,\\ldots, N$. We summarize the sets of training inputs in:\n","$$\n","\\boldsymbol X = %\\underbrace{\n","\\begin{bmatrix}\n","x_1^{(1)} & \\cdots & x_p^{(1)}  \\\\\n","x_1^{(2)} & \\cdots & x_p^{(2)}  \\\\\n","\\vdots & \\ddots & \\vdots \\\\\n","x_1^{(N)} & \\cdots & x_p^{(N)}  \\\\\n","\\end{bmatrix}\n","%}\n","_{N\\times p} \\qquad\n","\\boldsymbol y = %\\underbrace{\n","\\begin{bmatrix}\n","y^{(1)} \\\\ \\vdots \\\\ y^{(N)}\n","\\end{bmatrix} %}\n","_{N\\times1}\n","$$\n","**Note that for now we do not include the intercept $\\beta_0$.** We are interested in finding parameters $\\boldsymbol\\beta$ that map the inputs well to the ouputs according to the linear regression equation.\n","\n","From the lecture, we know that the parameters $\\boldsymbol\\beta$ found by the following equation are optimal:\n","$$\n","\\underset{\\boldsymbol\\beta}{\\text{min}} \\| \\boldsymbol y - \\boldsymbol X \\boldsymbol\\beta \\|^2\n","%= \\underset{\\boldsymbol\\beta}{\\text{min}} \\ \\text{L}_{\\text{LS}} (\\boldsymbol\\beta)\n","%where $\\text{L}_{\\text{LS}}$ is the (ordinary) least squares loss function.\n","$$\n","### Dataset generation\n","We will start with a simple training set, that we define by ourselves."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RvAyeBZort-_"},"outputs":[],"source":["# Define training set\n","X = np.array([-3, -1, 0, 1, 3]).reshape(-1,1) # 5x1 vector, N=5, D=1\n","y = np.array([-1.2, -0.7, 0.14, 0.67, 1.67]).reshape(-1,1) # 5x1 vector\n","\n","# Plot the training set\n","\n","plt.figure()\n","plt.plot(X, y, '+', markersize=10, label='Data')\n","plt.xlabel(\"$x$\")\n","plt.ylabel(\"$y$\")\n","plt.ylim(-5, 5)\n","plt.xlim(-5,5)\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"FXcgW-H4sJJ1"},"source":["<a name=\"section-2\"></a>\n","\n","## Section 2: Least squares loss and Maximum likelihood\n","\n","The solution of the the (ordinary) least squares is\n","$$\n","\\boldsymbol\\beta^{*} = (\\boldsymbol X^T\\boldsymbol X)^{-1}\\boldsymbol X^T\\boldsymbol y \\ \\in\\mathbb{R}^{p}\\,,\n","$$\n","\n","\n","The same estimate of $\\boldsymbol\\beta$ we can be obtained by maximum liklihood estimation which gives statistical interpretation of linear regression. In maximum likelihood estimation, we can find the parameters $\\boldsymbol\\beta^{\\mathrm{ML}}$ that maximize the likelihood\n","$$\n","p(\\boldsymbol y | \\boldsymbol X, \\boldsymbol\\beta) = \\prod_{i=1}^N p(y^{(i)} | \\boldsymbol x^{(i)}, \\boldsymbol\\beta)\\,.\n","$$\n","From the lecture we know that the maximum likelihood estimator is the same obtained from least squares:\n","$$\n","\\boldsymbol\\beta^{\\text{ML}} = \\boldsymbol\\beta^{*}= (\\boldsymbol X^T\\boldsymbol X)^{-1}\\boldsymbol X^T\\boldsymbol y \\, .\n","$$\n","\n","Let us compute $\\boldsymbol\\beta^{*}$  for the given training set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"517e-cosr6Om"},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def max_lik_estimate(X, y):\n","\n","    # X: N x p matrix of training inputs\n","    # y: N x 1 vector of training targets/observations\n","    # returns: maximum likelihood parameters (p x 1)\n","\n","    # check X and y have the same length\n","    assert X.shape[0] == y.shape[0], \"Input X and y have different lengths.\"\n","\n","    N, p = X.shape\n","    beta_ml = ... ## <-- EDIT THIS LINE\n","    return beta_ml"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"02HwjMK9tn3U"},"outputs":[],"source":["# get maximum likelihood estimate\n","beta_ml = max_lik_estimate(X,y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kbwz_2NW-6rn"},"outputs":[],"source":["beta_ml"]},{"cell_type":"markdown","metadata":{"id":"G7WtGsWZuMvX"},"source":["We see that there is only one parameter fitted which is the gradient. This will limit the fitted line to go through the origin (0,0). Now, make a prediction using the maximum likelihood estimate that we just found."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sTW4b5WJts8r"},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def predict_with_estimate(X_test, beta):\n","\n","    # X_test: K x p matrix of test inputs\n","    # beta: p x 1 vector of parameters\n","    # returns: prediction of f(X_test); K x 1 vector\n","\n","    # check X_test and beta have the same number of features\n","    assert X_test.shape[1] == beta.shape[0], \"X_test and beta have incompatible dimensions.\"\n","\n","    prediction = ... ## <-- EDIT THIS LINE\n","\n","    return prediction"]},{"cell_type":"markdown","metadata":{"id":"0tIKJwquu-av"},"source":["Let's see whether we got something useful:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vwOhNpGPuPnK"},"outputs":[],"source":["# define a test set\n","X_test = np.linspace(-5,5,100).reshape(-1,1) # 100 x 1 vector of test inputs\n","\n","# predict the function values at the test points using the maximum likelihood estimator\n","ml_prediction = predict_with_estimate(X_test, beta_ml)\n","\n","# plot\n","plt.figure()\n","plt.plot(X, y, '+', markersize=10, label='Data')\n","plt.plot(X_test, ml_prediction, label='Linear Regression Fit')\n","plt.xlabel(\"$x$\")\n","plt.ylabel(\"$y$\")\n","plt.ylim(-5, 5)\n","plt.xlim(-5,5)\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"rBC71Rw1vGDk"},"source":["#### Questions\n","1. Does the solution above look reasonable?\n","2. Play around with different values of $\\beta$. How do the corresponding functions change?\n","3. Modify the training targets $\\mathcal Y$ and re-run your computation. What changes?\n","\n","Let us now look at a different training set, where we add 2.0 to every $y$-value, and compute the maximum likelihood estimate."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aqOxB4BCvAn8"},"outputs":[],"source":["ynew = y + 2.0\n","\n","plt.figure()\n","plt.plot(X, ynew, '+', markersize=10, label='Data')\n","plt.xlabel(\"$x$\")\n","plt.ylabel(\"$y$\")\n","plt.ylim(-5, 5)\n","plt.xlim(-5,5)\n","plt.legend()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TpFRqKALvQ5w"},"outputs":[],"source":["# get maximum likelihood estimate\n","beta_ml = max_lik_estimate(X, ynew)\n","print(beta_ml)\n","\n","# define a test set\n","X_test = np.linspace(-5,5,100).reshape(-1,1) # 100 x 1 vector of test inputs\n","\n","# predict the function values at the test points using the maximum likelihood estimator\n","ml_prediction = predict_with_estimate(X_test, beta_ml)\n","\n","# plot\n","plt.figure()\n","plt.plot(X, ynew, '+', markersize=10, label='Data')\n","plt.plot(X_test, ml_prediction, label='Linear Regression Fit')\n","plt.ylim(-5, 5)\n","plt.xlim(-5,5)\n","plt.xlabel(\"$x$\")\n","plt.ylabel(\"$y$\")\n","plt.legend()"]},{"cell_type":"markdown","metadata":{"id":"3NrmAPSxvehP"},"source":["#### Question:\n","1. This maximum likelihood estimate doesn't look too good: The orange line is too far away from the observations although we just shifted them by 2. Why is this the case?\n","2. How can we fix this problem?\n","\n","Let us now define a linear regression model that is slightly more flexible:\n","$$\n","y = \\beta_0 + \\boldsymbol x^T \\boldsymbol\\beta_1 + \\epsilon\\,,\\quad \\epsilon\\sim\\mathcal N(0,\\sigma^2)\n","$$\n","\n","Here, we added an offset (also called bias or intercept) parameter $\\beta_0$ to our original model.\n","\n","#### Question:\n","1. What is the effect of this bias parameter, i.e., what additional flexibility does it offer?\n","\n","If we now define the inputs to be the augmented vector $\\boldsymbol x_{\\text{aug}} = \\begin{bmatrix}1\\\\\\boldsymbol x\\end{bmatrix}$, we can write the new linear regression model as\n","$$\n","y = \\boldsymbol x_{\\text{aug}}^T\\boldsymbol\\beta_{\\text{aug}} + \\epsilon\\,,\\quad \\boldsymbol\\beta_{\\text{aug}} = \\begin{bmatrix}\n","\\beta_0\\\\\n","\\boldsymbol\\beta_1\n","\\end{bmatrix}\\,.\n","$$\n","So we obtain a more general model including an intercept term $\\beta_0$. Hence we redefine $\\boldsymbol X$ starting from $\\boldsymbol x_{\\text{aug}}$, obtaining in this the $\\boldsymbol X$ used in the lecture notes:\n","$$\n","\\boldsymbol X = %\\underbrace{\n","\\begin{bmatrix}\n","1 & x_1^{(1)} & \\cdots & x_p^{(1)}  \\\\\n","1 & x_1^{(2)} & \\cdots & x_p^{(2)}  \\\\\n","1 & \\vdots & \\ddots & \\vdots \\\\\n","1 & x_1^{(N)} & \\cdots & x_p^{(N)}  \\\\\n","\\end{bmatrix}\n","%}\n","_{N\\times (p+1)}\n","$$\n","and we will use this definition of $\\boldsymbol X$ from now on."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WCxWSoilve2K"},"outputs":[],"source":["N, p = X.shape\n","X_aug = np.hstack([np.ones((N,1)), X]) # augmented training inputs of size N x (p+1)\n","beta_aug = np.zeros((p+1, 1)) # new beta vector of size (p+1) x 1"]},{"cell_type":"markdown","metadata":{"id":"7my_hrcewzl8"},"source":["Let us now compute the maximum likelihood estimator for this setting.\n","\n","_Hint:_ Re-use code that you have already written."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C-ZKNxhZwwNV"},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def max_lik_estimate_aug(X_aug, y):\n","\n","    beta_aug_ml = ... ## <-- EDIT THIS LINE\n","\n","    return beta_aug_ml"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HQLUCbzOw42V"},"outputs":[],"source":["beta_aug_ml = max_lik_estimate_aug(X_aug, ynew)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6c_49E00-6rq"},"outputs":[],"source":["beta_aug_ml # offset + slope"]},{"cell_type":"markdown","metadata":{"id":"DcPNFCh2w8wm"},"source":["We see that now there are two parameters fitted, the intercept and the gradient, which allowed more flexibilities. Now, we can make predictions again:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HBqFIZOUw6V2"},"outputs":[],"source":["# define a test set (we also need to augment the test inputs with ones)\n","X_test_aug = np.hstack([np.ones((X_test.shape[0],1)), X_test]) # 100 x (p + 1) vector of test inputs\n","\n","# predict the function values at the test points using the maximum likelihood estimator\n","ml_prediction = predict_with_estimate(X_test_aug, beta_aug_ml)\n","\n","# plot\n","plt.figure()\n","plt.plot(X, ynew, '+', markersize=10, label='Data')\n","plt.plot(X_test, ml_prediction, label='Linear Regression Fit')\n","plt.xlabel(\"$x$\")\n","plt.ylabel(\"$y$\")\n","plt.ylim(-5, 5)\n","plt.xlim(-5,5)\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"_pzbkWXTxC7w"},"source":["It seems this has solved our problem!\n","\n","#### Question:\n","1. Play around with the first parameter of $\\boldsymbol\\beta_{\\text{aug}}$ and see how the fit of the function changes.\n","2. Play around with the second parameter of $\\boldsymbol\\beta_{\\text{aug}}$ and see how the fit of the function changes."]},{"cell_type":"code","source":["beta_aug_ml_1 = beta_aug_ml.copy()\n","beta_aug_ml_1[0] += 0.5\n","ml_prediction = predict_with_estimate(X_test_aug, beta_aug_ml_1)\n","\n","# plot\n","plt.figure()\n","plt.plot(X, ynew, '+', markersize=10, label='Data')\n","plt.plot(X_test, ml_prediction, label='Linear Regression Fit')\n","plt.xlabel(\"$x$\")\n","plt.ylabel(\"$y$\")\n","plt.ylim(-5, 5)\n","plt.xlim(-5,5)\n","plt.legend()\n","plt.show()"],"metadata":{"id":"w7HoK6sAuvXc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["beta_aug_ml_2 = beta_aug_ml.copy()\n","beta_aug_ml_2[1] += 1\n","\n","ml_prediction = predict_with_estimate(X_test_aug, beta_aug_ml_2)\n","\n","# plot\n","plt.figure()\n","plt.plot(X, ynew, '+', markersize=10, label='Data')\n","plt.plot(X_test, ml_prediction, label='Linear Regression Fit')\n","plt.xlabel(\"$x$\")\n","plt.ylabel(\"$y$\")\n","plt.ylim(-5, 5)\n","plt.xlim(-5,5)\n","plt.legend()\n","plt.show()"],"metadata":{"id":"-hw4WyTCvRte"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XQEE_gSg__YI"},"source":["<a name=\"section-3\"></a>\n","\n","## Section 3: Ridge regression\n","\n","From our lectures, we know that ridge regression is an extension of linear regression with least squares loss function, including a (usually small) positive penalty term $\\lambda$:\n","$$\n","\\underset{\\boldsymbol\\beta}{\\text{min}} \\| \\boldsymbol y - \\boldsymbol X \\boldsymbol\\beta \\|^2 + \\lambda \\| \\boldsymbol\\beta \\|^2\n","$$\n","The solution is\n","$$\n","\\boldsymbol\\beta^{*}_{\\text{ridge}} = (\\boldsymbol X^T\\boldsymbol X + \\lambda I)^{-1}\\boldsymbol X^T\\boldsymbol y \\, .\n","$$\n","\n","\n","This time, we will define a very small training set of only two observations to demonstrate the advantages of ridge regression over least squares linear regression.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"afHJkiTmw_Hg"},"outputs":[],"source":["X_train = np.array([0.5, 1]).reshape(-1,1)\n","y_train = np.array([0.5, 1])\n","X_test = np.array([0, 2]).reshape(-1,1)"]},{"cell_type":"markdown","metadata":{"id":"NGe0EpTnAGuJ"},"source":["Let's define function similar to the one for least squares, but taking one additional argument, our penalty term $\\lambda$.\n","\n","_Hint_: we apply the same augmentation as above with least squares, so the offset is accurately captured."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3o1gM6sZAKk8"},"outputs":[],"source":["## EDIT THIS FUNCTION\n","def ridge_estimate(X, y, penalty):\n","\n","    # X: N x p matrix of training inputs\n","    # y: N x 1 vector of training targets/observations\n","    # returns: maximum likelihood parameters ((p + 1) x 1)\n","\n","    # check X and y have the same length\n","    assert X.shape[0] == y.shape[0], \"Input X and y have different lengths.\"\n","\n","    N, p = X.shape\n","    X_aug = np.hstack([np.ones((N,1)), X]) # augmented training inputs of size N x (p + 1)\n","    N_aug, p_aug = X_aug.shape\n","    I = np.identity(p_aug)\n","    I[0] = 0.0 # penalty excludes the bias term.\n","    beta_ridge = ... ## <-- EDIT THIS LINE\n","    return beta_ridge"]},{"cell_type":"markdown","metadata":{"id":"2pOO4NvoGLgB"},"source":["Now, we add a bit of Gaussian noise to our training set to mimic different realisations of the data sample and apply ridge regression. We should do it a couple of times (here 10 times) to obtain some statistics over the datasets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iJ18cd_kGKlO"},"outputs":[],"source":["penalty_term = 0.1 # lambda fixed for the realisations\n","fig, ax = plt.subplots(figsize=(12, 8))\n","X_test_aug = np.hstack([np.ones((X_test.shape[0],1)), X_test])\n","\n","# consider 10 noisy realisations\n","for i in range(10):\n","    # add Gaussian noise\n","    this_X = 0.1 * np.random.normal(size=(2, 1)) + X_train\n","\n","    # fit ridge regression on noisy realisation\n","    beta_ridge = ridge_estimate(this_X, y_train, penalty=penalty_term)\n","    ridge_prediction = predict_with_estimate(X_test_aug, beta_ridge)\n","\n","    ax.plot(X_test, ridge_prediction, color='gray',\n","            label='Ridge Regression Fit (on Noisy Data)' if i == 0 else '')\n","    ax.scatter(this_X, y_train, c='gray', marker='+', zorder=10,\n","               label='Noisy Data' if i == 0 else '')\n","\n","# fit ridge regression on original data\n","beta_ridge = ridge_estimate(X_train, y_train, penalty=penalty_term)\n","ridge_prediction_X = predict_with_estimate(X_test_aug, beta_ridge)\n","\n","ax.plot(X_test, ridge_prediction_X, linewidth=2, color='blue', label='Ridge Regression Fit')\n","ax.scatter(X_train, y_train, c='red', marker='+', zorder=10, label='Data')\n","plt.xlabel(\"$x$\")\n","plt.ylabel(\"$y$\")\n","plt.ylim(0, 2)\n","plt.xlim(0,2)\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"JCMTqIA9KkqX"},"source":["Let's compare this to ordinary least squares:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AKN-UR5jK3S0"},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(12, 8))\n","plt.xlim(0.0, 2)\n","plt.ylim(0.0, 2)\n","\n","X_train_aug = np.hstack([np.ones((X_test.shape[0],1)), X_train])\n","\n","X_test_aug = np.hstack([np.ones((X_test.shape[0],1)), X_test])\n","\n","# consider 10 noisy realisations\n","for i in range(10):\n","    # add Gaussian noise\n","    this_X = 0.1 * np.random.normal(size=(2, 1)) + X_train\n","    N, p = this_X.shape\n","    this_X_aug = np.hstack([np.ones((N,1)), this_X])\n","\n","    # fit ordinary LS regression on noisy realisation\n","    beta_aug_ml = max_lik_estimate_aug(this_X_aug, y_train)\n","    ml_prediction = predict_with_estimate(X_test_aug, beta_aug_ml)\n","\n","\n","    ax.plot(X_test, ml_prediction, color='gray',\n","            label='Unregularised Linear Regression Fit (on Noisy Data)' if i == 0 else '')\n","    ax.scatter(this_X, y_train, c='gray', marker='+', zorder=10,\n","               label = 'Noisy Data' if i == 0 else '')\n","\n","# fit ordinary LS regression on original data\n","beta_aug_ml = max_lik_estimate_aug(X_train_aug, y_train)\n","ml_prediction_X = predict_with_estimate(X_test_aug, beta_aug_ml)\n","\n","ax.plot(X_test, ml_prediction_X, linewidth=2, color='blue',\n","        label='Unregularised Linear Regression Fit')\n","ax.scatter(X_train, y_train, c='red', marker='+', zorder=10,\n","           label='Data')\n","plt.xlabel(\"$x$\")\n","plt.ylabel(\"$y$\")\n","plt.ylim(0, 2)\n","plt.xlim(0,2)\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"0-kn557FCcO_"},"source":["#### Questions\n","1. What differences between the two solutions above can you see?\n","2. **Optional**:\n","    - play around with different values of the penalty term $\\lambda$. How do the corresponding functions change? Which values provide the most reasonable results?\n","    - Can you replicate your results using [`sklearn.linear_model.Ridge`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)? [Note that this is only for comparison, sklearn is forbidden in the coursework].\n","    - Based on sklearn's documentation, can you see any differences in the algorithms that are implemented in sklearn?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pmTTGrf--6ru"},"outputs":[],"source":["penalty_term = 5 # larger lambda here\n","fig, ax = plt.subplots(figsize=(12, 8))\n","X_test_aug = np.hstack([np.ones((X_test.shape[0],1)), X_test])\n","\n","# consider 10 noisy realisations\n","for i in range(10):\n","    # add Gaussian noise\n","    this_X = 0.1 * np.random.normal(size=(2, 1)) + X_train\n","\n","    # fit ridge regression on noisy realisation\n","    beta_ridge = ridge_estimate(this_X, y_train, penalty=penalty_term)\n","    ridge_prediction = predict_with_estimate(X_test_aug, beta_ridge)\n","\n","    ax.plot(X_test, ridge_prediction, color='gray',\n","            label='Ridge Regression Fit (on Noisy Data)' if i == 0 else '')\n","    ax.scatter(this_X, y_train, c='gray', marker='+', zorder=10,\n","               label='Noisy Data' if i == 0 else '')\n","\n","# fit ridge regression on original data\n","beta_ridge = ridge_estimate(X_train, y_train, penalty=penalty_term)\n","ridge_prediction_X = predict_with_estimate(X_test_aug, beta_ridge)\n","\n","ax.plot(X_test, ridge_prediction_X, linewidth=2, color='blue', label='Ridge Regression Fit')\n","ax.scatter(X_train, y_train, c='red', marker='+', zorder=10, label='Data')\n","plt.xlabel(\"$x$\")\n","plt.ylabel(\"$y$\")\n","plt.ylim(0, 2)\n","plt.xlim(0,2)\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","source":["<a name=\"section-4\"></a>\n","\n","## Section 4: LASSO regression\n","\n","As opposed to the ridge regression which has a penalty term  $\\| \\boldsymbol\\beta \\|^2$, LASSO regression introduces $ \\| \\boldsymbol\\beta \\|_1 $, (also known as $L_1$ loss). $L_1$ loss is often preferred if we are interested in sparse parameters, i.e. few non-zero parameters. This is generally regarded as a feature selection task, and in high-dimensional problems it helps interpret the learned parameters and their relevance.\n","However, no closed-form solution exists for LASSO regression as in the standard and ridge regression, so we can use the iterative gradient-descent algorithm.\n","\n","\n","In LASSO regression the aim is to minimize the following loss:\n","\n","$L_\\text{LASSO}(\\boldsymbol{\\beta}) = || \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\beta}||^2 + \\lambda ||\\boldsymbol{\\beta}||_1$\n","\n","Where $||\\boldsymbol{\\beta}||_1 = \\sum_{i=1}^p |\\beta_i|$\n","\n","\n","The absolute function $|.|$ adds nonsmoothness to the loss function, which can prevent the gradient-descent to converge properly to the optimal solution, and will keep bouncing around it instead. To solve this, we can use the Huber loss as an alternative to the $L_1$ loss; it combines the behaviour of $L_1$ loss to a quadratic function around the zero to obtain a `relaxed' form of the same optimisation.\n","\n","Here one replaces $||\\boldsymbol{\\beta}||_1$ with the Huber Loss $\\sum_{i=1}^p L_c(\\beta_i)$, where $L_c(\\beta)$ is defined as:\n","\n","$L_c (\\beta) =\n","\\begin{cases}\n"," \\frac{1}{2}{\\beta^2}                   & \\text{for } |\\beta| \\le c, \\\\\n"," c (|\\beta| - \\frac{1}{2}c), & \\text{otherwise.}\n","\\end{cases}\n","$\n","\n","The $c$ parameter in Huber determines the range around zero with $L_2$-like behaviour to ensure smoothness and, hence, better convergence.\n","\n","The piecewise smooth function $L_c (\\beta)$ has the gradient:\n","\n","$\\frac{dL_c (\\beta)}{d\\beta} =\n","  \\begin{cases}\n"," \\beta                   & \\text{for } |\\beta| \\le c, \\\\\n"," c\\, \\text{sgn}(\\beta) , & \\text{otherwise.}\n","\\end{cases}\n","$\n","\n","Now we can minimise the following relaxed function by gradient descent:\n","\n","$\n","\\begin{align} L_\\text{LASSO-Huber}(\\boldsymbol{\\beta})\n","&= || \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\beta}||^2 + \\lambda \\sum_{i=1}^p L_c(\\beta) \\\\\n","&= (\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta})^T(\\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta})  + \\lambda \\sum_{i=1}^p L_c(\\beta) \\\\\n","&= \\left(\\boldsymbol{y}^T\\boldsymbol{y} - \\boldsymbol{y}^T\\boldsymbol{X}\\boldsymbol{\\beta} - \\boldsymbol{\\beta}^T\\boldsymbol{X}^T  \\boldsymbol{y} + \\boldsymbol{\\beta}^T\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{\\beta}\\right) + \\lambda \\sum_{i=1}^p L_c(\\beta)\n","\\end{align}\n","$\n","\n","Which has the gradient:\n","\n","$\n","\\begin{align} \\nabla_\\boldsymbol{\\beta} L_\\text{LASSO-Huber}\n","&= 2\\left(\\boldsymbol{X}^T\\boldsymbol{X}\\boldsymbol{\\beta} - \\boldsymbol{X}^T\\boldsymbol{y}\\right) + \\lambda \\nabla_{\\boldsymbol{\\beta}}L_c(\\boldsymbol{\\beta})\n","\\end{align}$\n","\n","Optimisation method:\n","- **Initialise** $\\boldsymbol{\\beta}$ with zeros.\n","- Use Gradient-descent for tuning $\\boldsymbol{\\beta}$\n"],"metadata":{"id":"TPMo_S-v9d8w"}},{"cell_type":"markdown","source":["Implementated in Python as:"],"metadata":{"id":"aNv5eHEoFk4x"}},{"cell_type":"code","source":["# huber loss\n","def huber(beta, c = 1e-6):\n","    return np.where(np.abs(beta) < c, (beta**2)/2.,c * (np.abs(beta) - c/2))\n","\n","# gradient of huber loss\n","def grad_huber(beta, c = 1e-6):\n","    g = np.empty_like(beta)\n","    return  np.where(np.abs(beta) < c, beta, c * np.sign(beta))"],"metadata":{"id":"88D42ZKp-jre"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# range to plot over\n","a = np.linspace(-1, 1, 1000)\n","plt.plot(a, huber(a, c=0.1))\n","plt.xlabel(\"$x$\")\n","plt.ylabel(\"$L_{c}(x)$\")\n","plt.ylim(0, 0.1)\n","plt.xlim(-1, 1)\n","plt.show()"],"metadata":{"id":"mFqwFuVMFttr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# range to plot over\n","a = np.linspace(-1, 1, 1000)\n","plt.plot(a, grad_huber(a, c=0.1))\n","plt.xlabel(\"$x$\")\n","plt.ylabel(r'$ \\nabla L_{c}(x)$') # The r in r'$ \\nabla L_{LASSO-Huber}(x)$' is a prefix indicating a raw string. Read if interested: https://www.digitalocean.com/community/tutorials/python-raw-string\n","plt.ylim(-0.15, 0.15)\n","plt.xlim(-1, 1)\n","plt.show()"],"metadata":{"id":"xjkfe7FUGnPG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Question: Try different $c$ values and observe the difference"],"metadata":{"id":"Xo8FNl7rGe5X"}},{"cell_type":"markdown","source":["### Optimisation with gradient-descent\n","\n","We next implement gradient-descent to solve the optimisation for the LASSO model. Recall from the lecture notes that gradient descent is an algorithm that follows iteratively the direction of $-\\nabla_{\\boldsymbol \\beta}L(\\boldsymbol \\beta)$, where $L(\\boldsymbol \\beta)$ is the loss to be minimsed (here $L_\\text{LASSO-Huber}(\\boldsymbol{\\beta})$). At the  iteration $t+1$ the parameters are updated according to:\n","$$\n","\\boldsymbol \\beta_{t+1} = \\boldsymbol \\beta_t - \\eta_t \\nabla_{\\boldsymbol \\beta} L(\\boldsymbol \\beta_t)\n","$$\n","where $\\eta_t$ is the step size."],"metadata":{"id":"y5oTp7s7HGvQ"}},{"cell_type":"code","source":["def minimize_ls_huber(X, y, lambd, n_iters = 10000, step_size=5e-5, c_huber=1e-4):\n","    \"\"\"\n","    This function estimates the regression parameters with the relaxed version\n","    of LASSO regression using the gradient-descent algorithm to find the optimal\n","    solution.\n","    Args:\n","    X (np.array): The augmented data matrix with shape (N, p + 1).\n","    y (np.array): The response column with shape (N, 1).\n","    lambd (float): The multiplier of the relaxed L1 term.\n","    n_iters (int): Number of gradient descent iterations.\n","    step_size (float): The step size in the updating step.\n","    \"\"\"\n","\n","    # check X and y have the same length\n","    assert X.shape[0] == y.shape[0], \"Input X and y have different lengths.\"\n","\n","    N, p = X.shape\n","    # Precomputed products to avoid redundant computations.\n","    XX = X.T @ X\n","    Xy = X.T @ y\n","    # Initialize beta params with zeros\n","    beta = np.zeros(p)\n","\n","    for i in range(n_iters):\n","        # Compute the gradient of the relaxed LASSO, Huber.\n","        grad_c = grad_huber(beta, c=c_huber)\n","\n","        # Intercept term is not involved in the regularisation.\n","        grad_c[-1] = ... ## <-- EDIT THIS LINE\n","\n","        # Compute the gradient of the regularised loss.\n","        grad = ... ## <-- EDIT THIS LINE\n","\n","        # Update beta\n","        beta = ... ## <-- EDIT THIS LINE\n","\n","    return beta"],"metadata":{"id":"J34BcLIkF6TQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To study the feature selection capability of LASSO, we generate a 3 dimensional synthetic data set $X$ where the second dimension does not contribute significantly to the target $y$."],"metadata":{"id":"AiH908N-6GBf"}},{"cell_type":"code","source":["np.random.seed(42)\n","\n","# Generate random data with three features\n","X1 = np.random.rand(100)\n","X2 = np.random.rand(100)  # Insignificant feature (with small beta below)\n","X3 = np.random.rand(100)\n","true_beta = np.array([8, 0.5, 9])  # Weights for features\n","y = X1 * true_beta[0] + X2 * true_beta[1] + X3 * true_beta[2] + np.random.randn(100)"],"metadata":{"id":"C35ZSCXDKMLR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can compare the ground truth coefficients used to create the synthetic data set to the optimal LASSO coefficients. We observe that the insignificant second feature has an optimal LASSO coefficent close to zero, and this illustrates that LASSO only selects the first and third feature as significant."],"metadata":{"id":"y3jlAEBW7eb4"}},{"cell_type":"code","source":["# Add bias term\n","X_aug = np.stack((X1, X2, X3, np.ones(100)), axis=1)\n","\n","# Run LASSO regression\n","beta_lasso = minimize_ls_huber(X_aug, y, 5e3, n_iters=15000,\n","                                step_size=1e-5,\n","                                c_huber=1e-5)\n","\n","# Print the result\n","print(\"LASSO Regression Coefficients:\")\n","print(\"Beta (Intercept):\", beta_lasso[3])\n","print(\"Beta 1:\", beta_lasso[0])\n","print(\"Beta 2:\", beta_lasso[1])\n","print(\"Beta 3:\", beta_lasso[2])"],"metadata":{"id":"76hFOV9yKOSW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","for beta, label in ((true_beta, 'True'), (beta_lasso[:-1], 'LASSO')):\n","    plt.bar([1,2,3], beta, color='blue' if label == 'True' else 'green')\n","    plt.xlabel('Feature Index')\n","    plt.xticks([1,2,3])\n","    plt.ylabel(f'{label} Coefficients')\n","    plt.title(f'{label} Coefficients of Features')\n","    plt.show()"],"metadata":{"id":"F9i3ops6SAib"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Questions\n","\n","1. Try adding more **insignificant** variables and repeat the experiments. Do you still get the expected sparse solution? If not, what hyperparameters might need a re-tune?\n","\n","2. Can you observe a clear pattern in how coefficients change as the penalty term $\\lambda$ varies? Can you create a plot that shows the trajectory of each coefficient as $\\lambda$ changes?\n","\n","3. **Optional**:\n","    - Can you replicate your results using [`sklearn.linear_model.Lasso`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso)?\n","    - Based on sklearn's documentation, can you see any differences in the algorithms that are implemented in sklearn?\n","\n"],"metadata":{"id":"ZDkTcY7xR2zC"}},{"cell_type":"markdown","source":["#### Answers\n","\n","\n","We repeat the experiment on a five dimensional dataset $X$ where the second and fifth dimensions contribute insignicantly to the target $y$."],"metadata":{"id":"o6-IbhOT8RQd"}},{"cell_type":"code","source":["np.random.seed(0)\n","\n","# Generate random data with five features\n","X1 = np.random.rand(100)\n","X2 = np.random.rand(100)  # Insignificant feature (with small beta below)\n","X3 = np.random.rand(100)\n","X4 = np.random.rand(100)\n","X5 = np.random.rand(100) # Insignificant feature (with small beta below)\n","\n","true_beta = np.array([8, 0.5, 9, 4, 0.01])  # Weights for features\n","\n","y = X1 * true_beta[0] + X2 * true_beta[1] + X3 * true_beta[2] +  X4 * true_beta[3] +  X5 * true_beta[4] + np.random.randn(100)\n","# Add bias term\n","X_aug = np.stack((X1, X2, X3, X4, X5, np.ones(100)), axis=1)"],"metadata":{"id":"eg1IMLwnikLY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can now study the effect of the penality term $\\lambda$ on the LASSO coefficients."],"metadata":{"id":"GsH3-Tqu-LlX"}},{"cell_type":"code","source":["def lasso_coefficient_trajectories(lambdas, X_aug, y):\n","    \"\"\"Computes the trajectories of the LASSO coeffiencts for a list of lambdas.\n","    \"\"\"\n","    # initialise trajectories\n","    coeff_trajectories = []\n","    for lam in lambdas:\n","        # Run LASSO regression\n","        beta_lasso = minimize_ls_huber(X_aug, y, lambd=lam, n_iters=15000,\n","                                        step_size=1e-5,\n","                                        c_huber=1e-5)\n","        coeff_trajectories.append(beta_lasso[:-1])\n","    return np.vstack(coeff_trajectories)"],"metadata":{"id":"5Hzuphpek2n8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define range for lambdas\n","lambdas = np.logspace(-5, 5, 40) * 200\n","\n","# compute the trajectories of LASSO coefficients\n","coeffs = lasso_coefficient_trajectories(lambdas, X_aug, y)"],"metadata":{"id":"6fb2vTAnk4EL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plotting the trajectories of the LASSO coefficients for the 5 different features below shows us that the insiginifcant features 2 and 5 are always assigned LASSO coefficients close to zero. Only when the regularization is very strong, i.e., for large values of $\\lambda$, all coefficients are pushed towards zero."],"metadata":{"id":"5o6nxmK6_1br"}},{"cell_type":"code","source":["# plot the trajectores of LASSO coefficients using log-scale\n","plt.figure(figsize=(12, 8))\n","for i in range(coeffs.shape[1]):\n","    plt.plot(np.log10(lambdas), coeffs[:, i], label=f'Feature {i + 1}')\n","\n","plt.xlabel('log($\\lambda$)')\n","plt.ylabel('Coefficient Value')\n","plt.title('Trajectories of LASSO Coefficients for Different Features')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"H1k5ohuYnfbV"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.10"}},"nbformat":4,"nbformat_minor":0}